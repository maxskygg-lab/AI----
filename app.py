import streamlit as st
import sys
import os
import time
import tempfile
import arxiv
import requests
import math
import re
from streamlit_agraph import agraph, Node, Edge, Config

# ================= 1. ç¯å¢ƒå¬è¯Šå™¨ =================
try:
    import zhipuai
    import langchain_community
    import fitz  # pymupdf
except ImportError as e:
    st.error(f"ğŸš‘ ç¯å¢ƒç¼ºå¤±åº“ -> {e.name}")
    st.stop()
# ===============================================

from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import ZhipuAIEmbeddings
from langchain_community.chat_models import ChatZhipuAI
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ================= 2. é¡µé¢é…ç½® =================
st.set_page_config(page_title="AI æ·±åº¦ç ”è¯»åŠ©æ‰‹ (ä¸“ä¸šè°ƒç ”ç‰ˆ)", layout="wide", page_icon="ğŸ“")
st.markdown("""
<style>
    .stButton>button {width: 100%; border-radius: 8px;}
    .reportview-container { margin-top: -2em; }
    .abstract-box {
        background-color: #f0f2f6;
        padding: 15px;
        border-radius: 8px;
        border-left: 5px solid #4CAF50;
        font-size: 0.95em;
        line-height: 1.6;
        margin-bottom: 10px;
        color: #31333F;
    }
    .cite-badge {
        background-color: #ff4b4b;
        color: white;
        padding: 2px 8px;
        border-radius: 12px;
        font-size: 0.8em;
        font-weight: bold;
    }
    .detail-panel {
        background-color: #ffffff;
        padding: 20px;
        border-radius: 10px;
        border: 1px solid #ddd;
        height: 600px;
        overflow-y: auto;
    }
</style>
""", unsafe_allow_html=True)
st.title("ğŸ“– AI æ·±åº¦ç ”è¯»åŠ©æ‰‹ (ä¸“ä¸šè°ƒç ”ç‰ˆ)")

# ================= 3. çŠ¶æ€åˆå§‹åŒ– =================
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "db" not in st.session_state:
    st.session_state.db = None
if "loaded_files" not in st.session_state:
    st.session_state.loaded_files = []
if "all_chunks" not in st.session_state:
    st.session_state.all_chunks = []
if "suggested_query" not in st.session_state:
    st.session_state.suggested_query = ""
if "search_results" not in st.session_state:
    st.session_state.search_results = []
if "selected_scope" not in st.session_state:
    st.session_state.selected_scope = "ğŸŒ å¯¹æ¯”æ‰€æœ‰è®ºæ–‡"
if "focus_paper_id" not in st.session_state: 
    st.session_state.focus_paper_id = None

# ================= 4. æ ¸å¿ƒé€»è¾‘å‡½æ•° =================

def get_pure_arxiv_id(url):
    """ä» URL ä¸­ç²¾å‡†æå– ArXiv ID"""
    match = re.search(r'(\d{4}\.\d{4,5})', url)
    if match:
        return match.group(1)
    return url.split('/')[-1].split('v')[0]

def fetch_citations(arxiv_id, ss_key=None):
    """ä» Semantic Scholar API è·å–å¼•ç”¨æ•°"""
    try:
        clean_id = get_pure_arxiv_id(arxiv_id)
        api_url = f"https://api.semanticscholar.org/graph/v1/paper/ArXiv:{clean_id}?fields=citationCount"
        headers = {"x-api-key": ss_key} if ss_key else {}
        response = requests.get(api_url, headers=headers, timeout=5)
        if response.status_code == 200:
            return response.json().get('citationCount', 0)
    except:
        pass
    return 0

@st.cache_data(ttl=3600)
def fetch_graph_data(arxiv_id, ss_key=None):
    """è·å–å…³è”æ•°æ®ï¼ˆä¸¥æ ¼æ³¨å…¥å­çº§æ‘˜è¦å­—æ®µï¼‰"""
    try:
        clean_id = get_pure_arxiv_id(arxiv_id)
        # ç²¾å‡†ä¿®å¤ç‚¹ï¼šç¡®ä¿ references å’Œ citations çš„å†…éƒ¨ä¹Ÿæœ‰ abstract
        fields = "paperId,title,year,citationCount,abstract,references.paperId,references.title,references.citationCount,references.year,references.abstract,citations.paperId,citations.title,citations.citationCount,citations.year,citations.abstract"
        api_url = f"https://api.semanticscholar.org/graph/v1/paper/ArXiv:{clean_id}?fields={fields}"
        headers = {"x-api-key": ss_key} if ss_key else {}
        
        if not ss_key:
            time.sleep(1.5) # åŒ¿åé™æµä¿æŠ¤
            
        response = requests.get(api_url, headers=headers, timeout=10)
        if response.status_code == 200:
            return response.json()
        return None
    except Exception as e:
        st.error(f"å›¾è°±æ•°æ®æŠ“å–å¤±è´¥: {e}")
        return None

def render_connected_graph(data):
    """æ¸²æŸ“å›¾è°±ï¼ˆæ¢å¤åŒå‘ç¾¤ç°‡é€»è¾‘ï¼‰"""
    if not data: 
        return None, {}
    
    nodes, edges = [], []
    paper_details = {} 
    
    # æ ¸å¿ƒè®ºæ–‡
    seed_id = data.get('paperId', 'root')
    seed_title = data.get('title', 'Seed Paper')
    paper_details[seed_id] = {
        "title": seed_title,
        "abstract": data.get('abstract', 'æ— æ‘˜è¦ä¿¡æ¯'),
        "year": data.get('year', 'Unknown'),
        "cites": data.get('citationCount', 0)
    }
    nodes.append(Node(id=seed_id, label="â­ SEED", size=30, color="#FF4B4B"))

    seen_ids = {seed_id}
    # æ¢å¤ references å’Œ citations ä¸¤ä¸ªç¾¤ç°‡çš„æå–
    for rel_type in ['references', 'citations']:
        items = data.get(rel_type, [])[:15]
        for p in items:
            p_id = p.get('paperId')
            if not p_id or p_id in seen_ids:
                continue
            seen_ids.add(p_id)
            
            title = p.get('title', 'Unknown')
            paper_details[p_id] = {
                "title": title,
                "abstract": p.get('abstract', 'è¯¥å…³è”æ–‡çŒ®æš‚æ— è¯¦ç»†æ‘˜è¦ã€‚'),
                "year": p.get('year', 'Unknown'),
                "cites": p.get('citationCount', 0)
            }
            
            c_count = p.get('citationCount', 0)
            node_size = 12 + (math.log2(c_count + 1) * 4)
            node_color = "#3b82f6" if rel_type == 'references' else "#10b981"
            
            nodes.append(Node(id=p_id, label=f"{title[:15]}...", size=node_size, color=node_color))
            if rel_type == 'references':
                edges.append(Edge(source=seed_id, target=p_id, color="#3b82f6", width=1))
            else:
                edges.append(Edge(source=p_id, target=seed_id, color="#10b981", width=1))

    config = Config(width="100%", height=600, directed=True, physics=True, nodeHighlightBehavior=True, highlightColor="#F7D154")
    clicked_id = agraph(nodes=nodes, edges=edges, config=config)
    return clicked_id, paper_details

def fix_latex_errors(text):
    if not text: return text
    text = text.replace(r"\(", "$").replace(r"\)", "$")
    text = text.replace(r"\[", "$$").replace(r"\]", "$$")
    return text

def rebuild_index_from_chunks(api_key):
    if not st.session_state.all_chunks:
        st.session_state.db = None
        return
    embeddings = ZhipuAIEmbeddings(model="embedding-2", api_key=api_key)
    st.session_state.db = FAISS.from_documents(st.session_state.all_chunks, embeddings)

def process_and_add_to_db(file_path, file_name, api_key):
    try:
        loader = PyPDFLoader(file_path)
        docs = loader.load()
        for doc in docs:
            doc.metadata['source_paper'] = file_name
        
        splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=200, separators=["\n\n", "\n", "ã€‚", ".", " ", ""])
        chunks = splitter.split_documents(docs)
        valid_chunks = [c for c in chunks if len(c.page_content.strip()) > 20]
        
        st.session_state.all_chunks.extend(valid_chunks)
        embeddings = ZhipuAIEmbeddings(model="embedding-2", api_key=api_key)
        
        # åˆ†æ‰¹å¤„ç†é˜²æ­¢è¶…æ—¶
        batch_size = 10
        total = len(valid_chunks)
        if st.session_state.db is None:
            st.session_state.db = FAISS.from_documents(valid_chunks[:batch_size], embeddings)
            if total > batch_size:
                for i in range(batch_size, total, batch_size):
                    st.session_state.db.add_documents(valid_chunks[i: i + batch_size])
                    time.sleep(0.1)
        else:
            for i in range(0, total, batch_size):
                st.session_state.db.add_documents(valid_chunks[i: i + batch_size])
                time.sleep(0.1)
        
        if file_name not in st.session_state.loaded_files:
            st.session_state.loaded_files.append(file_name)
        st.session_state.chat_history.append({"role": "system_notice", "content": f"ğŸ“š **ç³»ç»Ÿé€šçŸ¥**ï¼šå·²åŠ è½½ã€Š{file_name}ã€‹ã€‚"})
    except Exception as e:
        st.error(f"å¤„ç†å¤±è´¥: {e}")

# ================= 5. ä¾§è¾¹æ å¸ƒå±€ =================
with st.sidebar:
    st.header("ğŸ›ï¸ åŠ©æ‰‹æ§åˆ¶å°")
    user_api_key = st.text_input("æ™ºè°± AI API Key", type="password", help="ç”¨äºå¤§æ¨¡å‹å¯¹è¯å’Œå‘é‡åŒ–")
    ss_api_key = st.text_input("Semantic Scholar Key (å¯é€‰)", type="password", help="å¡«å…¥å¯æé«˜æ¥å£è°ƒç”¨é¢‘ç‡é™åˆ¶")
    st.markdown("---")
    
    if st.session_state.loaded_files:
        st.subheader("ğŸ—‚ï¸ å·²åŠ è½½æ–‡çŒ®")
        for file in list(st.session_state.loaded_files):
            col_f1, col_f2 = st.columns([4, 1])
            with col_f1:
                st.text(f"ğŸ“„ {file[:18]}..." if len(file)>20 else f"ğŸ“„ {file}")
            with col_f2:
                if st.button("ğŸ—‘ï¸", key=f"del_{file}"):
                    st.session_state.loaded_files.remove(file)
                    st.session_state.all_chunks = [c for c in st.session_state.all_chunks if c.metadata.get('source_paper') != file]
                    if user_api_key:
                        rebuild_index_from_chunks(user_api_key)
                    st.rerun()
        
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰æ–‡çŒ®", type="primary"):
            st.session_state.db = None
            st.session_state.loaded_files = []
            st.session_state.all_chunks = []
            st.session_state.chat_history = []
            st.rerun()

    st.subheader("âš™ï¸ ç ”è¯»åå¥½")
    reading_mode = st.radio("å¯¹è¯æ¨¡å¼:", ["ğŸŸ¢ å¿«é€Ÿå›ç­”", "ğŸ“– é€æ®µç²¾è¯» (å¢å¼ºå…¬å¼)"], index=1)

    if st.session_state.loaded_files:
        st.markdown("---")
        if st.button("ğŸª„ è‡ªåŠ¨ç”Ÿæˆå¤šè®ºæ–‡å¯¹æ¯”è¡¨"):
            if user_api_key and st.session_state.db:
                with st.spinner("æ·±åº¦åˆ†æä¸­..."):
                    llm = ChatZhipuAI(model="glm-4", api_key=user_api_key, temperature=0.1)
                    aggregated_context = ""
                    for filename in st.session_state.loaded_files:
                        sub_docs = st.session_state.db.similarity_search("Abstract and main findings", k=2, filter={"source_paper": filename})
                        if sub_docs:
                            aggregated_context += f"\n=== æ–‡çŒ®: {filename} ===\n" + "\n".join([d.page_content for d in sub_docs])
                    res = llm.invoke(f"åŸºäºä»¥ä¸‹å†…å®¹ç”Ÿæˆ Markdown å¯¹æ¯”è¡¨ï¼š\n{aggregated_context}")
                    st.session_state.chat_history.append({"role": "assistant", "content": res.content})
                    st.rerun()

        scope_options = ["ğŸŒ å¯¹æ¯”æ‰€æœ‰è®ºæ–‡"] + st.session_state.loaded_files
        st.session_state.selected_scope = st.selectbox("ğŸ‘ï¸ å½“å‰å¯¹è¯èŒƒå›´", scope_options)

    st.markdown("---")
    st.subheader("ğŸ“¥ æœ¬åœ°è®ºæ–‡ä¸Šä¼ ")
    uploaded_file = st.file_uploader("ä¸Šä¼  PDF æ–‡çŒ®", type="pdf")
    if uploaded_file and user_api_key and st.button("å¼€å§‹è§£æå¹¶å­¦ä¹ "):
        with st.spinner("PDF æ·±åº¦è§£æä¸­..."):
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
                tmp.write(uploaded_file.getvalue())
                path = tmp.name
            process_and_add_to_db(path, uploaded_file.name, user_api_key)
            os.remove(path)
            st.rerun()

# ================= 6. ä¸»ç•Œé¢å¸ƒå±€ =================
tab_search, tab_chat = st.tabs(["ğŸ” æ–‡çŒ®è°ƒç ” (Connected Papers æ¨¡å¼)", "ğŸ’¬ è®ºæ–‡æ·±è¯»ç©ºé—´"])

with tab_search:
    st.subheader("ğŸŒ ArXiv å…¨çƒæ–‡çŒ®æ£€ç´¢")
    col_q, col_sort, col_n = st.columns([3, 1.5, 1])
    with col_q:
        search_query = st.text_input("æ£€ç´¢å…³é”®è¯", placeholder="ä¾‹å¦‚: 'transformer architecture' æˆ– 'LLM reasoning'")
    with col_sort:
        sort_mode = st.selectbox("æ’åºæ–¹å¼", ["ğŸ”¥ ç›¸å…³æ€§ä¼˜å…ˆ", "ğŸ“… æœ€æ–°å‘å¸ƒ", "ğŸ“ˆ å¼•ç”¨é‡ä¹‹æœ€"])
    with col_n:
        max_results = st.number_input("è·å–æ•°é‡", min_value=5, max_value=50, value=15)
        
    if st.button("ğŸš€ æ‰§è¡Œæ£€ç´¢") and search_query:
        with st.spinner("æ­£åœ¨æ£€ç´¢å¹¶æ‹‰å–å¼•ç”¨ç»Ÿè®¡ä¿¡æ¯..."):
            try:
                arxiv_sort = arxiv.SortCriterion.Relevance
                if "æœ€æ–°" in sort_mode: arxiv_sort = arxiv.SortCriterion.SubmittedDate
                search = arxiv.Search(query=search_query, max_results=max_results, sort_by=arxiv_sort)
                raw_results = list(search.results())
                results_with_cite = []
                for res in raw_results:
                    cites = fetch_citations(res.entry_id, ss_key=ss_api_key)
                    results_with_cite.append({'obj': res, 'citations': cites})
                if "å¼•ç”¨é‡" in sort_mode:
                    results_with_cite.sort(key=lambda x: x['citations'], reverse=True)
                st.session_state.search_results = results_with_cite
                st.success(f"âœ… æ‰¾åˆ° {len(results_with_cite)} ç¯‡ç›¸å…³æ–‡çŒ®")
            except Exception as e:
                st.error(f"æ£€ç´¢å¤±è´¥: {e}")
                
    if st.session_state.search_results:
        # å›¾è°±æ¸²æŸ“é¢æ¿
        if st.session_state.focus_paper_id:
            st.markdown("---")
            st.subheader("ğŸ“Š æ–‡çŒ®å…³è”ç½‘ç»œ (Connected Graph)")
            g_data = fetch_graph_data(st.session_state.focus_paper_id, ss_key=ss_api_key)
            if not g_data:
                st.warning("âš ï¸ æ— æ³•è·å–å›¾è°±æ•°æ®ã€‚å¦‚æœæ˜¯åŒ¿åæ¨¡å¼ï¼Œè¯·ç¨åå†è¯•æˆ–å¡«å…¥ SS API Keyã€‚")
            else:
                col_graph, col_info = st.columns([2.5, 1])
                with col_graph:
                    clicked_node_id, all_details = render_connected_graph(g_data)
                with col_info:
                    if clicked_node_id and clicked_node_id in all_details:
                        info = all_details[clicked_node_id]
                        st.markdown(f"### ğŸ“„ é€‰å®šæ–‡çŒ®è¯¦æƒ…")
                        st.markdown(f"**æ ‡é¢˜**: {info['title']}")
                        st.markdown(f"**å¹´ä»½**: {info['year']} | **å¼•ç”¨**: {info['cites']}")
                        st.markdown("---")
                        st.markdown(f'<div class="abstract-box">{info["abstract"]}</div>', unsafe_allow_html=True)
                    else:
                        st.info("ğŸ’¡ **æ“ä½œæç¤º**\n\nç‚¹å‡»å·¦ä¾§åœ†ç‚¹å³å¯åœ¨æ­¤å¤„æŸ¥çœ‹å¯¹åº”è®ºæ–‡çš„æ‘˜è¦ã€‚")
                        if st.button("âŒ å…³é—­å›¾è°±é¢æ¿"):
                            st.session_state.focus_paper_id = None
                            st.rerun()
            st.markdown("---")

        for i, item in enumerate(st.session_state.search_results):
            res = item['obj']
            cites = item['citations']
            with st.expander(f"#{i+1} ğŸ“„ {res.title} ({res.published.year})"):
                st.markdown(f"**ğŸ”¥ å¼•ç”¨æ¬¡æ•°**: <span class='cite-badge'>{cites}</span>", unsafe_allow_html=True)
                st.write(res.summary.replace("\n", " "))
                col1, col2, col3 = st.columns([1, 1, 1])
                with col1:
                    st.markdown(f"[ğŸ”— ArXiv é¡µé¢]({res.entry_id})")
                with col2:
                    if st.button(f"â¬‡ï¸ åŠ å…¥æ·±è¯»åº“", key=f"dl_search_{i}"):
                        if user_api_key:
                            with st.spinner("ä¸‹è½½å¹¶è§£æä¸­..."):
                                pdf_path = res.download_pdf(dirpath=tempfile.gettempdir())
                                process_and_add_to_db(pdf_path, res.title, user_api_key)
                                st.success("å·²æˆåŠŸå…¥åº“")
                        else:
                            st.error("è¯·å…ˆåœ¨ä¾§è¾¹æ å¡«å…¥ API Key")
                with col3:
                    if st.button(f"ğŸ•¸ï¸ æŸ¥çœ‹å…³ç³»ç¾¤", key=f"btn_graph_{i}"):
                        st.session_state.focus_paper_id = res.entry_id
                        st.rerun()

with tab_chat:
    if st.session_state.loaded_files:
        st.caption(f"ğŸ“š èŒƒå›´ï¼š{st.session_state.selected_scope} | æ¨¡å¼ï¼š{reading_mode}")
    for msg in st.session_state.chat_history:
        if msg["role"] == "system_notice":
            st.info(msg["content"])
        else:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])
                
    if prompt := st.chat_input("åŸºäºå·²åŠ è½½çš„æ–‡çŒ®æé—®..."):
        if not st.session_state.db:
            st.warning("âš ï¸ è¯·å…ˆåœ¨ä¾§è¾¹æ ä¸Šä¼ è®ºæ–‡æˆ–ä»æ£€ç´¢ç»“æœä¸­ä¸‹è½½è®ºæ–‡ã€‚")
        else:
            st.session_state.chat_history.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.write(prompt)
            with st.chat_message("assistant"):
                try:
                    scope = st.session_state.selected_scope
                    f_dict = {"source_paper": scope} if scope != "ğŸŒ å¯¹æ¯”æ‰€æœ‰è®ºæ–‡" else None
                    docs = st.session_state.db.similarity_search(prompt, k=8, filter=f_dict)
                    context = "\n\n".join([f"ğŸ“„ã€{d.metadata.get('source_paper','?')}ã€‘:\n{d.page_content}" for d in docs])
                    
                    llm = ChatZhipuAI(model="glm-4", api_key=user_api_key)
                    response = llm.invoke(f"èƒŒæ™¯èµ„æ–™ï¼š\n{context}\n\né—®é¢˜ï¼š{prompt}")
                    
                    final_content = fix_latex_errors(response.content)
                    st.write(final_content)
                    st.session_state.chat_history.append({"role": "assistant", "content": final_content})
                except Exception as e:
                    st.error(f"å¯¹è¯å¼•æ“æ•…éšœ: {e}")
