import streamlit as st
import sys
import os
import time
import tempfile
import arxiv
import requests
import re
from collections import Counter

# ================= 1. ç¯å¢ƒæ£€æŸ¥ =================
try:
    import zhipuai
    import langchain_community
    import fitz  # pymupdf
except ImportError as e:
    st.error(f"ğŸš‘ ç¯å¢ƒç¼ºå¤±åº“ -> {e.name}")
    st.stop()

from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import ZhipuAIEmbeddings
from langchain_community.chat_models import ChatZhipuAI
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ================= 2. é¡µé¢é…ç½® =================
st.set_page_config(page_title="AI æ·±åº¦ç ”è¯»åŠ©æ‰‹ (Google-Style è°ƒç ”ç‰ˆ)", layout="wide", page_icon="ğŸ“")
st.markdown("""
<style>
    .stButton>button {width: 100%; border-radius: 8px;}
    .abstract-box {
        background-color: #f8f9fa;
        padding: 15px;
        border-radius: 8px;
        border-left: 5px solid #4CAF50;
        font-size: 0.95em;
        line-height: 1.6;
        margin-bottom: 10px;
    }
    .cite-badge {
        background-color: #e74c3c;
        color: white;
        padding: 2px 10px;
        border-radius: 20px;
        font-size: 0.85em;
        font-weight: bold;
    }
    .metric-card {
        background-color: #ffffff;
        padding: 10px;
        border: 1px solid #e0e0e0;
        border-radius: 5px;
        text-align: center;
    }
</style>
""", unsafe_allow_html=True)
st.title("ğŸ“– AI æ·±åº¦ç ”è¯»åŠ©æ‰‹ (ä¸“ä¸šç§‘ç ”å¼•æ“)")

# ================= 3. çŠ¶æ€åˆå§‹åŒ– =================
for key in ["chat_history", "loaded_files", "all_chunks", "suggested_query", "search_results"]:
    if key not in st.session_state:
        st.session_state[key] = []
if "db" not in st.session_state:
    st.session_state.db = None
if "selected_scope" not in st.session_state:
    st.session_state.selected_scope = "ğŸŒ å¯¹æ¯”æ‰€æœ‰è®ºæ–‡"

# ================= 4. æ ¸å¿ƒé€»è¾‘å·¥å…· =================

def fetch_citations(arxiv_id):
    """æ¥å…¥ Semantic Scholar å¼•ç”¨å…³ç³»æ•°æ®"""
    try:
        clean_id = arxiv_id.split('/')[-1].split('v')[0]
        api_url = f"https://api.semanticscholar.org/graph/v1/paper/ArXiv:{clean_id}?fields=citationCount"
        response = requests.get(api_url, timeout=3)
        if response.status_code == 200:
            return response.json().get('citationCount', 0)
    except:
        pass
    return 0

def extract_top_topics(results):
    """å­¦ä¹ è°·æ­Œçš„å…³é”®è¯æå–ï¼Œç”¨äºè¾…åŠ©åˆ¤æ–­è°ƒç ”æ–¹å‘"""
    all_text = ""
    for item in results:
        res = item['obj']
        all_text += f" {res.title} {res.summary}"
    
    words = re.findall(r'\b\w{5,}\b', all_text.lower())
    stop_words = {'learning', 'robotics', 'education', 'research', 'paper', 'approach', 'system', 'based', 'using', 'results'}
    meaningful_words = [w for w in words if w not in stop_words]
    return Counter(meaningful_words).most_common(8)

def fix_latex_errors(text):
    if not text: return text
    text = text.replace(r"\(", "$").replace(r"\)", "$")
    text = text.replace(r"\[", "$$").replace(r"\]", "$$")
    return text

def process_and_add_to_db(file_path, file_name, api_key):
    try:
        loader = PyPDFLoader(file_path)
        docs = loader.load()
        for doc in docs:
            doc.metadata['source_paper'] = file_name
        splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=200)
        chunks = splitter.split_documents(docs)
        embeddings = ZhipuAIEmbeddings(model="embedding-2", api_key=api_key)
        if st.session_state.db is None:
            st.session_state.db = FAISS.from_documents(chunks, embeddings)
        else:
            st.session_state.db.add_documents(chunks)
        if file_name not in st.session_state.loaded_files:
            st.session_state.loaded_files.append(file_name)
        st.session_state.chat_history.append({"role": "system_notice", "content": f"ğŸ“š å·²åŠ è½½ã€Š{file_name}ã€‹"})
    except Exception as e:
        st.error(f"è§£æå¤±è´¥: {e}")

# ================= 5. ä¾§è¾¹æ  =================
with st.sidebar:
    st.header("âš™ï¸ ç§‘ç ”æ§åˆ¶å°")
    user_api_key = st.text_input("API Key (æ™ºè°±)", type="password")
    
    if st.session_state.loaded_files:
        st.subheader("ğŸ—‚ï¸ æœ¬åœ°æ–‡çŒ®åº“")
        for file in list(st.session_state.loaded_files):
            col_f1, col_f2 = st.columns([4, 1])
            with col_f1: st.caption(f"ğŸ“„ {file[:20]}...")
            with col_f2:
                if st.button("ğŸ—‘ï¸", key=f"del_{file}"):
                    st.session_state.loaded_files.remove(file)
                    st.rerun()
        
        st.session_state.selected_scope = st.selectbox("ğŸ‘ï¸ ä¸“æ³¨èŒƒå›´", ["ğŸŒ å¯¹æ¯”æ‰€æœ‰è®ºæ–‡"] + st.session_state.loaded_files)

    st.markdown("---")
    st.subheader("ğŸ“¥ å¯¼å…¥æ–‡çŒ®")
    uploaded_file = st.file_uploader("ä¸Šä¼  PDF", type="pdf")
    if uploaded_file and user_api_key and st.button("å¼€å§‹åˆ†æ"):
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(uploaded_file.getvalue())
            process_and_add_to_db(tmp.name, uploaded_file.name, user_api_key)
            os.remove(tmp.name)
            st.rerun()

# ================= 6. ä¸»ç•Œé¢ =================
tab_search, tab_chat = st.tabs(["ğŸ” æ–‡çŒ®è°ƒç ”å¼•æ“", "ğŸ’¬ äº¤äº’ç ”è¯»ç©ºé—´"])

with tab_search:
    st.subheader("ğŸŒ ArXiv + Semantic Scholar è”åˆæœç´¢")
    col_q, col_sort, col_n = st.columns([3, 1.2, 0.8])
    with col_q:
        q = st.text_input("å…³é”®è¯", value=st.session_state.suggested_query, placeholder="å¦‚: education robot review")
    with col_sort:
        sort_rule = st.selectbox("æ’åºè§„åˆ™", ["ğŸ”¥ ç›¸å…³æ€§", "ğŸ“… æ—¶é—´", "ğŸ“ˆ å¼•ç”¨é‡"])
    with col_n:
        n = st.number_input("æ•°é‡", 5, 50, 15)

    if st.button("ğŸ” æ·±åº¦æ£€ç´¢") and q:
        with st.spinner("è°·æ­Œå¼å¤šç»´æ£€ç´¢ä¸­..."):
            try:
                # 1. æ‰§è¡Œ ArXiv æ£€ç´¢
                arxiv_sort = arxiv.SortCriterion.Relevance
                if "æ—¶é—´" in sort_rule: arxiv_sort = arxiv.SortCriterion.SubmittedDate
                
                # å¸ƒå°”é€»è¾‘è‡ªåŠ¨å¢å¼º
                refined_q = q
                if " " in q and "AND" not in q:
                    refined_q = " AND ".join([f"(ti:{w} OR abs:{w})" for w in q.split()])
                
                search = arxiv.Search(query=refined_q, max_results=n, sort_by=arxiv_sort)
                raw_results = list(search.results())
                
                # 2. è¡¥å…¨å¼•ç”¨æ•°æ®
                results_with_meta = []
                for res in raw_results:
                    cites = fetch_citations(res.entry_id)
                    results_with_meta.append({'obj': res, 'citations': cites})
                
                # 3. å¼•ç”¨æ’åº
                if "å¼•ç”¨é‡" in sort_rule:
                    results_with_meta.sort(key=lambda x: x['citations'], reverse=True)
                
                st.session_state.search_results = results_with_meta
            except Exception as e:
                st.error(f"æ£€ç´¢å‡ºé”™: {e}")

    # --- æ¸²æŸ“è°ƒç ”æŒ‡æ ‡ (Google Knowledge Graph é€»è¾‘) ---
    if st.session_state.search_results:
        topics = extract_top_topics(st.session_state.search_results)
        st.markdown("---")
        st.write("ğŸ“Š **å½“å‰æœç´¢ç»“æœé¢†åŸŸçƒ­ç‚¹ç»Ÿè®¡** (æœ‰åŠ©äºè¯†åˆ«ç ”ç©¶åå‘):")
        cols = st.columns(8)
        for i, (word, count) in enumerate(topics):
            cols[i].markdown(f"<div class='metric-card'><b>{word}</b><br>{count}æ¬¡</div>", unsafe_allow_html=True)
        
        st.markdown("---")
        # --- æ¸²æŸ“åˆ—è¡¨ ---
        for i, item in enumerate(st.session_state.search_results):
            res = item['obj']
            cites = item['citations']
            
            # åˆ¤æ–­æ˜¯å¦é«˜åº¦ç›¸å…³ï¼ˆæ ‡é¢˜åŒ¹é…ï¼‰
            title_match = any(word in res.title.lower() for word in q.lower().split())
            
            with st.expander(f"{'ğŸ¯' if title_match else 'ğŸ“„'} #{i+1} {res.title} ({res.published.year})"):
                st.markdown(f"**ğŸ”¥ å¼•ç”¨é‡:** <span class='cite-badge'>{cites}</span> | **ä½œè€…:** {res.authors[0].name} ç­‰", unsafe_allow_html=True)
                st.markdown(f"<div class='abstract-box'>{res.summary.replace(chr(10), ' ')}</div>", unsafe_allow_html=True)
                
                col1, col2 = st.columns([1, 1])
                with col1: st.markdown(f"[ğŸ”— åŸæ–‡åœ°å€]({res.entry_id})")
                with col2:
                    if st.button(f"â¬‡ï¸ ç ”è¯»æ­¤ç¯‡", key=f"dl_{i}"):
                        if user_api_key:
                            with st.spinner("å…¥åº“ä¸­..."):
                                path = res.download_pdf(dirpath=tempfile.gettempdir())
                                process_and_add_to_db(path, res.title, user_api_key)
                                st.success("å·²åŠ å…¥ç ”è¯»ç©ºé—´ï¼")
                        else: st.error("è¯·é…ç½® API Key")

with tab_chat:
    if st.session_state.loaded_files:
        st.caption(f"ğŸ“š ä¸“æ³¨è®ºæ–‡: {st.session_state.selected_scope}")

    for msg in st.session_state.chat_history:
        if msg["role"] == "system_notice": st.info(msg["content"])
        else:
            with st.chat_message(msg["role"]): st.markdown(msg["content"])

    if prompt := st.chat_input("é’ˆå¯¹å·²é€‰è®ºæ–‡æé—®..."):
        if not st.session_state.db: st.warning("è¯·å…ˆåœ¨æœç´¢ç»“æœä¸­ç‚¹å‡»â€˜ç ”è¯»æ­¤ç¯‡â€™æˆ–ä¸Šä¼  PDF")
        else:
            st.session_state.chat_history.append({"role": "user", "content": prompt})
            with st.chat_message("user"): st.write(prompt)
            with st.chat_message("assistant"):
                try:
                    scope = st.session_state.selected_scope
                    f_dict = {"source_paper": scope} if scope != "ğŸŒ å¯¹æ¯”æ‰€æœ‰è®ºæ–‡" else None
                    docs = st.session_state.db.similarity_search(prompt, k=8, filter=f_dict)
                    context = "\n\n".join([f"[{d.metadata.get('source_paper','?')}] {d.page_content}" for d in docs])
                    
                    llm = ChatZhipuAI(model="glm-4", api_key=user_api_key, temperature=0.1)
                    res = llm.invoke(f"èƒŒæ™¯èµ„æ–™ï¼š\n{context}\n\né—®é¢˜ï¼š{prompt}\nè¦æ±‚ï¼šå­¦æœ¯ä¸¥è°¨ï¼Œå…¬å¼ç”¨ $ åŒ…è£¹ã€‚")
                    ans = fix_latex_errors(res.content)
                    st.write(ans)
                    st.session_state.chat_history.append({"role": "assistant", "content": ans})
                except Exception as e: st.error(f"å¯¹è¯å¼‚å¸¸: {e}")
