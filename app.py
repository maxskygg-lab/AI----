import streamlit as st
import sys
import os
import time
import tempfile
import arxiv
import requests
import re
from collections import Counter

# ================= 1. ç¯å¢ƒè‡ªæ£€ä¸æ ¸å¿ƒå¯¼å…¥ =================
try:
    import zhipuai
    import langchain_community
    import fitz  # pymupdf
except ImportError as e:
    st.error(f"ğŸš‘ ç¯å¢ƒç¼ºå¤±æ ¸å¿ƒåº“ -> {e.name}ã€‚è¯·æ‰§è¡Œ: pip install zhipuai langchain_community pymupdf requests arxiv")
    st.stop()

from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import ZhipuAIEmbeddings
from langchain_community.chat_models import ChatZhipuAI
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ================= 2. é¡µé¢é…ç½®ä¸è°·æ­Œå¼ä¸“ä¸š CSS (å…¨é‡æ¢å¤) =================
st.set_page_config(page_title="AI æ·±åº¦ç ”è¯»åŠ©æ‰‹ (å…¨åŠŸèƒ½å…¨é‡ç‰ˆ)", layout="wide", page_icon="ğŸ“")
st.markdown("""
<style>
    .stButton>button { width: 100%; border-radius: 8px; font-weight: bold; transition: 0.3s; }
    .search-card {
        background-color: white; padding: 22px; border-radius: 10px;
        margin-bottom: 18px; border: 1px solid #dfe1e5;
        box-shadow: 0 1px 3px rgba(0,0,0,0.04);
    }
    .search-card:hover { box-shadow: 0 4px 12px rgba(32,33,36,0.18); border-color: rgba(223,225,229,0); }
    .paper-title { color: #1a0dab; font-size: 1.25em; text-decoration: none; font-weight: 500; display: block; margin-bottom: 4px; }
    .paper-url { color: #006621; font-size: 0.88em; margin-bottom: 8px; white-space: nowrap; overflow: hidden; text-overflow: ellipsis; }
    .snippet { color: #4d5156; font-size: 0.92em; line-height: 1.6; display: -webkit-box; -webkit-line-clamp: 3; -webkit-box-orient: vertical; overflow: hidden; }
    .cite-count { color: #70757a; font-size: 0.85em; font-weight: bold; background: #f8f9fa; padding: 3px 10px; border-radius: 5px; border: 1px solid #f1f3f4; }
    .abstract-box {
        background-color: #f8f9fa; padding: 20px; border-radius: 12px;
        border-left: 6px solid #28a745; font-size: 0.98em; line-height: 1.8;
        margin-bottom: 15px; color: #3c4043;
    }
    .topic-tag { 
        display: inline-block; background-color: #f1f3f4; color: #3c4043; 
        padding: 5px 14px; border-radius: 20px; margin: 5px; font-size: 0.88em; font-weight: 500;
    }
</style>
""", unsafe_allow_html=True)
st.title("ğŸ“– AI æ·±åº¦ç ”è¯»åŠ©æ‰‹ (Google Logic å…¨åŠŸèƒ½ç‰ˆ)")

# ================= 3. å…¨å±€çŠ¶æ€ä¸¥æ ¼åˆå§‹åŒ– =================
state_keys = {
    "chat_history": [],
    "db": None,
    "loaded_files": [],
    "all_chunks": [],
    "suggested_query": "",
    "search_results": [],
    "selected_scope": "ğŸŒ å¯¹æ¯”æ‰€æœ‰è®ºæ–‡"
}
for key, default in state_keys.items():
    if key not in st.session_state:
        st.session_state[key] = default

# ================= 4. æ ¸å¿ƒåŠŸèƒ½å‡½æ•°é›† (å…¨é‡é€»è¾‘æ¢å¤) =================

def clean_query_for_arxiv(raw_query):
    """ã€è°·æ­Œé€»è¾‘ 1ã€‘å°†é•¿è¯ç»„ç²¾å‡†æ‹†è§£ä¸ºå¸ƒå°”æ£€ç´¢å¼ï¼Œè§£å†³æœç´¢ä¸å‡ºè®ºæ–‡çš„é—®é¢˜"""
    words = re.sub(r'[^\w\s]', '', raw_query).split()
    stops = {'the', 'a', 'of', 'and', 'in', 'on', 'with', 'for', 'research', 'paper', 'study', 'impact'}
    important_words = [w for w in words if w.lower() not in stops and len(w) > 2]
    if not important_words: return raw_query
    # ä¼˜å…ˆæœç´¢æ ‡é¢˜ï¼Œæ‘˜è¦å…œåº•ï¼Œå–å‰ 4 ä¸ªæ ¸å¿ƒè¯ç»„
    query_parts = [f"(ti:{w} OR abs:{w})" for w in important_words[:4]]
    return " AND ".join(query_parts)

def fetch_citations(arxiv_id):
    """ä» Semantic Scholar è·å–å®æ—¶å¼•ç”¨é‡æ•°æ®"""
    try:
        clean_id = arxiv_id.split('/')[-1].split('v')[0]
        api_url = f"https://api.semanticscholar.org/graph/v1/paper/ArXiv:{clean_id}?fields=citationCount"
        response = requests.get(api_url, timeout=5)
        if response.status_code == 200:
            return response.json().get('citationCount', 0)
    except Exception: pass
    return 0

def extract_top_topics(results):
    """å­¦æœ¯çƒ­ç‚¹è¯åˆ†æé€»è¾‘"""
    all_text = " ".join([f"{r['obj'].title} {r['obj'].summary}" for r in results])
    words = re.findall(r'\b\w{5,}\b', all_text.lower())
    stop_words = {'learning', 'robotics', 'education', 'research', 'paper', 'approach', 'system', 'based', 'using', 'results', 'model'}
    meaningful = [w for w in words if w not in stop_words]
    return Counter(meaningful).most_common(10)

def fix_latex_errors(text):
    """LaTeX å…¬å¼æ·±åº¦ä¿®å¤é€»è¾‘"""
    if not text: return text
    text = text.replace(r"\(", "$").replace(r"\)", "$")
    text = text.replace(r"\[", "$$").replace(r"\]", "$$")
    return text

def generate_html_report(chat_history):
    """ã€æ¢å¤å…¨é‡é€»è¾‘ã€‘å¯¼å‡ºå¸¦æ ·å¼ä¸ MathJax çš„ HTML ç¬”è®°"""
    html = """<!DOCTYPE html><html><head><meta charset="UTF-8">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body { font-family: 'Segoe UI', Tahoma, Geneva, sans-serif; max-width: 900px; margin: 0 auto; padding: 40px; line-height: 1.7; color: #333; background-color: #fdfdfd; }
        h1 { color: #1b5e20; border-bottom: 3px solid #4caf50; padding-bottom: 15px; }
        .message { margin-bottom: 30px; padding: 25px; border-radius: 15px; box-shadow: 0 2px 8px rgba(0,0,0,0.06); }
        .user { background-color: #e3f2fd; border-left: 8px solid #1976d2; }
        .assistant { background-color: #f1f8e9; border-left: 8px solid #43a047; }
        .system { background-color: #fff3e0; border-left: 8px solid #fb8c00; font-style: italic; color: #666; }
        .role { font-weight: bold; display: block; margin-bottom: 12px; text-transform: uppercase; font-size: 0.85em; color: #555; }
    </style></head><body><h1>ğŸ“ AI æ·±åº¦ç ”è¯»ç¬”è®°</h1>"""
    for msg in chat_history:
        role_label = "ğŸ§‘â€ğŸ’» USER" if msg['role'] == 'user' else "ğŸ¤– AI RESEARCHER" if msg['role'] == 'assistant' else "ğŸ”” SYSTEM"
        html += f'<div class="message {msg["role"]}"><span class="role">{role_label}</span>{msg["content"].replace(chr(10), "<br>")}</div>'
    html += "</body></html>"
    return html

def rebuild_index_from_chunks(api_key):
    """ç‰©ç†é‡æ„å‘é‡ç´¢å¼• (å¸¦ Batch ä¿æŠ¤)"""
    if not st.session_state.all_chunks:
        st.session_state.db = None
        return
    embeddings = ZhipuAIEmbeddings(model="embedding-2", api_key=api_key)
    chunks = st.session_state.all_chunks
    batch_size = 30
    st.session_state.db = FAISS.from_documents(chunks[:batch_size], embeddings)
    for i in range(batch_size, len(chunks), batch_size):
        st.session_state.db.add_documents(chunks[i:i+batch_size])
        time.sleep(0.1)

def process_and_add_to_db(file_path, file_name, api_key):
    """ã€å½»åº•ä¿®å¤ 1214 é”™è¯¯ã€‘æ˜¾å¼å¾ªç¯åˆ†æ‰¹å¤„ç†é€»è¾‘ï¼Œä¸å†ç®€å†™"""
    try:
        loader = PyPDFLoader(file_path)
        docs = loader.load()
        for doc in docs: doc.metadata['source_paper'] = file_name
        splitter = RecursiveCharacterTextSplitter(chunk_size=750, chunk_overlap=200)
        new_chunks = splitter.split_documents(docs)
        valid_new = [c for c in new_chunks if len(c.page_content.strip()) > 30]
        
        embeddings = ZhipuAIEmbeddings(model="embedding-2", api_key=api_key)
        batch_size = 30
        total = len(valid_new)
        
        with st.spinner(f"æ­£åœ¨åˆ†æ‰¹å‘é‡åŒ–ã€Š{file_name}ã€‹ï¼Œç»•è¿‡æ¥å£é™åˆ¶..."):
            if st.session_state.db is None:
                st.session_state.db = FAISS.from_documents(valid_new[:batch_size], embeddings)
                start_idx = batch_size
            else: start_idx = 0
            
            for i in range(start_idx, total, batch_size):
                st.session_state.db.add_documents(valid_new[i : i + batch_size])
                time.sleep(0.2)
        
        st.session_state.all_chunks.extend(valid_new)
        if file_name not in st.session_state.loaded_files: st.session_state.loaded_files.append(file_name)
        st.session_state.chat_history.append({"role": "system_notice", "content": f"ğŸ“š å·²æˆåŠŸå…¥åº“: {file_name}"})
    except Exception as e: st.error(f"è§£æå¤±è´¥: {e}")

# ================= 5. ä¾§è¾¹æ ï¼šæ§åˆ¶é¢æ¿ (å…¨é‡åŠŸèƒ½å¤åŸ) =================
with st.sidebar:
    st.header("ğŸ›ï¸ ç§‘ç ”æ§åˆ¶å°")
    api_key_input = st.text_input("æ™ºè°± API Key", type="password")
    st.markdown("---")
    
    if st.session_state.loaded_files:
        st.subheader("ğŸ—‚ï¸ æ–‡çŒ®åº“ç®¡ç†")
        for f in list(st.session_state.loaded_files):
            c1, c2 = st.columns([4, 1])
            with c1: st.caption(f"ğŸ“„ {f[:22]}...")
            with c2:
                if st.button("ğŸ—‘ï¸", key=f"del_{f}"):
                    st.session_state.loaded_files.remove(f)
                    st.session_state.all_chunks = [c for c in st.session_state.all_chunks if c.metadata.get('source_paper') != f]
                    if api_key_input: rebuild_index_from_chunks(api_key_input)
                    st.rerun()

        if st.button("ğŸª„ ä¸€é”®ç”Ÿæˆç»¼è¿°å¯¹æ¯”è¡¨", type="primary"):
            if api_key_input and st.session_state.db:
                with st.spinner("æ·±åº¦æ‰«ææ–‡çŒ®ç‰¹å¾ä¸­..."):
                    llm = ChatZhipuAI(model="glm-4", api_key=api_key_input, temperature=0.1)
                    agg_ctx = ""
                    for name in st.session_state.loaded_files:
                        subs = st.session_state.db.similarity_search("Abstract methodology", k=2, filter={"source_paper": name})
                        agg_ctx += f"\n[Paper: {name}]\n" + "\n".join([d.page_content for d in subs])
                    res = llm.invoke(f"åˆ†æä»¥ä¸‹æ–‡çŒ®ç‰‡æ®µï¼Œç”Ÿæˆ Markdown å¯¹æ¯”è¡¨æ ¼(åŒ…å«ï¼šè®ºæ–‡åã€æ ¸å¿ƒåˆ›æ–°ç‚¹ã€ç ”ç©¶æ–¹æ³•ã€ç»“è®º)ï¼š\n{agg_ctx}")
                    st.session_state.chat_history.append({"role": "assistant", "content": res.content})
                    st.rerun()

        st.markdown("---")
        st.session_state.selected_scope = st.selectbox("ğŸ‘ï¸ å¯¹è¯ä¸“æ³¨èŒƒå›´", ["ğŸŒ å¯¹æ¯”æ‰€æœ‰è®ºæ–‡"] + st.session_state.loaded_files)

    st.markdown("---")
    if st.session_state.chat_history:
        st.download_button("ğŸ’¾ ä¸‹è½½ç ”è¯»æŠ¥å‘Š (HTMLå…¨æ ·å¼)", generate_html_report(st.session_state.chat_history), "research_notes.html", "text/html")
    
    up_pdf = st.file_uploader("å¯¼å…¥ PDF è®ºæ–‡", type="pdf")
    if up_pdf and api_key_input and st.button("å¼€å§‹è¯†åˆ«å…¥åº“"):
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as t:
            t.write(up_pdf.getvalue())
            process_and_add_to_db(t.name, up_pdf.name, api_key_input)
            os.remove(t.name)
            st.rerun()

# ================= 6. ä¸»ç•Œé¢ï¼šè°·æ­Œå¼æ£€ç´¢ä¸ç ”è¯» =================
tab_search, tab_chat = st.tabs(["ğŸ” è°·æ­Œå¼ç§‘ç ”è°ƒç ”", "ğŸ’¬ æ·±åº¦ç ”è¯»å¯¹è¯"])

with tab_search:
    st.subheader("ğŸŒ è°·æ­Œé€»è¾‘åŠ æƒæ£€ç´¢å¼•æ“")
    col_q, col_s, col_n = st.columns([3, 1.2, 0.8])
    with col_q: 
        q_input = st.text_input("å…³é”®è¯", value=st.session_state.suggested_query, placeholder="è¾“å…¥è¯¾é¢˜å…³é”®è¯")
    with col_s: 
        sort_rule = st.selectbox("è°·æ­Œæ’åºæƒé‡", ["ç»¼åˆæ’åº (è°·æ­Œæ¨¡å¼)", "å¼•ç”¨é‡ä¼˜å…ˆ", "æœ€æ–°å‘å¸ƒä¼˜å…ˆ"])
    with col_n: 
        n_count = st.number_input("è·å–ç¯‡æ•°", 5, 50, 15)

    if st.button("ğŸš€ æ‰§è¡Œå¤šç»´æ£€ç´¢") and q_input:
        with st.spinner("åŒæ­¥è·¨åº“æ•°æ®å¹¶è®¡ç®—æƒé‡ä¸­..."):
            try:
                arxiv_sort = arxiv.SortCriterion.Relevance
                if "æœ€æ–°" in sort_rule: arxiv_sort = arxiv.SortCriterion.SubmittedDate
                
                # ã€è°·æ­Œé€»è¾‘ã€‘å¸ƒå°”è½¬æ¢
                search_q = clean_query_for_arxiv(q_input)
                search_client = arxiv.Search(query=search_q, max_results=n_count, sort_by=arxiv_sort)
                
                final_results = []
                for res in list(search_client.results()):
                    cite_count = fetch_citations(res.entry_id)
                    # ã€è°·æ­Œä¼˜ç‚¹ 2ã€‘æ’åºç®—æ³•ï¼šæ ‡é¢˜åŒ¹é…æƒé‡ + å¼•ç”¨æƒé‡
                    title_weight = 100 if any(w.lower() in res.title.lower() for w in q_input.split()) else 0
                    score = cite_count * 2.5 + title_weight
                    final_results.append({'obj': res, 'cite': cite_count, 'score': score})
                    time.sleep(0.1)
                
                if "ç»¼åˆ" in sort_rule: final_results.sort(key=lambda x: x['score'], reverse=True)
                elif "å¼•ç”¨" in sort_rule: final_results.sort(key=lambda x: x['cite'], reverse=True)
                
                st.session_state.search_results = final_results
            except Exception as e: st.error(f"æ£€ç´¢å¤±è´¥: {e}")

    if st.session_state.search_results:
        # ã€è°·æ­Œä¼˜ç‚¹ 3ã€‘çƒ­ç‚¹åˆ†å¸ƒæ‘˜è¦
        topics = extract_top_topics(st.session_state.search_results)
        st.write("ğŸ“Š **å½“å‰è°ƒç ”çƒ­ç‚¹èšç±»ç»Ÿè®¡:**")
        tp_cols = st.columns(len(topics))
        for i, (w, c) in enumerate(topics): tp_cols[i].markdown(f"<div class='topic-tag'>{w} ({c})</div>", unsafe_allow_html=True)
        
        st.markdown("---")
        for i, item in enumerate(st.session_state.search_results):
            res, cite = item['obj'], item['cite']
            # ã€è°·æ­Œä¼˜ç‚¹ 4ã€‘æ¨¡æ‹Ÿè°·æ­Œå¡ç‰‡ UIï¼Œæ˜¾ç¤ºæ‘˜è¦ç‰‡æ®µ
            st.markdown(f"""
            <div class="search-card">
                <a class="paper-title" href="{res.entry_id}" target="_blank">{res.title}</a>
                <div class="paper-url">{res.entry_id}</div>
                <div class="snippet">{res.summary[:350].replace(chr(10), ' ')}...</div>
                <div style="margin-top:12px;">
                    <span class="cite-count">ğŸ“ˆ {cite} å¼•ç”¨</span>
                    <span style="margin-left:15px; color:#70757a; font-size:0.85em;">ğŸ“… {res.published.year} | {res.authors[0]} ç­‰</span>
                </div>
            </div>
            """, unsafe_allow_html=True)
            if st.button(f"â¬‡ï¸ åŠ å…¥ç ”è¯»åº“", key=f"dl_btn_{i}"):
                if api_key_input:
                    with st.spinner("åŒæ­¥å‘é‡åŒ–ä¸­..."):
                        p_path = res.download_pdf(dirpath=tempfile.gettempdir())
                        process_and_add_to_db(p_path, res.title, api_key_input)
                        st.success("å…¥åº“æˆåŠŸï¼")
                else: st.error("è¯·å¡«å…¥ API Key")

with tab_chat:
    if st.session_state.loaded_files:
        st.caption(f"ğŸ“š ä¸“æ³¨èŒƒå›´: {st.session_state.selected_scope}")
        for msg in st.session_state.chat_history:
            if msg["role"] == "system_notice": st.info(msg["content"])
            else:
                with st.chat_message(msg["role"]): st.markdown(msg["content"])

        if p_input := st.chat_input("åŸºäºæ–‡çŒ®æé—®..."):
            st.session_state.chat_history.append({"role": "user", "content": p_input})
            with st.chat_message("user"): st.write(p_input)
            with st.chat_message("assistant"):
                try:
                    scope = st.session_state.selected_scope
                    f_dict = {"source_paper": scope} if scope != "ğŸŒ å¯¹æ¯”æ‰€æœ‰è®ºæ–‡" else None
                    # MMR æ·±åº¦æ£€ç´¢é€»è¾‘è¿˜åŸ
                    docs = st.session_state.db.max_marginal_relevance_search(p_input, k=8, fetch_k=20, lambda_mult=0.7, filter=f_dict)
                    ctx = "\n\n".join([f"ğŸ“„ã€{d.metadata.get('source_paper','?')}ã€‘:\n{d.page_content}" for d in docs])
                    llm = ChatZhipuAI(model="glm-4", api_key=api_key_input, temperature=0.1)
                    full_res = llm.invoke(f"ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„ç§‘ç ”ä¸“å®¶ã€‚åŸºäºèµ„æ–™å›ç­”ï¼š\n{ctx}\né—®é¢˜ï¼š{p_input}\nè¦æ±‚ï¼šå­¦æœ¯ä¸¥è°¨ï¼Œå…¬å¼åŠ¡å¿…ç”¨ $ åŒ…è£¹ã€‚")
                    final_txt = fix_latex_errors(full_res.content)
                    st.write(final_txt)
                    st.session_state.chat_history.append({"role": "assistant", "content": final_txt})
                except Exception as e: st.error(f"å¯¹è¯å¼‚å¸¸: {e}")

        # æŒ–æ˜åŠŸèƒ½æŒ‰é’®è¿˜åŸ
        if st.button("ğŸ” æŒ–æ˜å½“å‰è¯¾é¢˜çš„å…³è”æ–°è®ºæ–‡"):
            if api_key_input and st.session_state.chat_history:
                with st.spinner("AI æ­£åœ¨è§£æè¯­ä¹‰ç‰¹å¾..."):
                    llm = ChatZhipuAI(model="glm-4", api_key=api_key_input)
                    context_bits = str(st.session_state.chat_history[-2:])
                    # ã€è°·æ­Œé€»è¾‘è¡¥å…¨ã€‘å¼•å¯¼ AI ç”Ÿæˆç®€çŸ­çš„ã€æ£€ç´¢å‹å¥½çš„è¯ç»„
                    prompt = f"æ ¹æ®ä»¥ä¸‹ç ”è¯»è®°å½•ï¼Œæå– 2 ä¸ªç®€ç»ƒçš„è‹±æ–‡å­¦æœ¯æœç´¢è¯ç»„ï¼Œç”¨äºè¿›ä¸€æ­¥è°ƒç ”ï¼ˆä¸¥ç¦é•¿å¥ï¼Œåªè¾“å‡ºè¯ç»„ï¼‰ï¼š\n{context_bits}"
                    st.session_state.suggested_query = llm.invoke(prompt).content.strip()
                    st.success(f"å·²ç”Ÿæˆè°·æ­Œå¼è¯ç»„ï¼š{st.session_state.suggested_query}ï¼Œè¯·å»è°ƒç ” Tab æœç´¢ã€‚")
    else:
        st.info("ğŸ’¡ ç ”è¯»åº“ä¸ºç©ºã€‚è¯·å…ˆé€šè¿‡è°ƒç ”å¼•æ“ä¸‹è½½è®ºæ–‡ï¼Œæˆ–æ‰‹åŠ¨ä¸Šä¼  PDFã€‚")
