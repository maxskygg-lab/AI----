import streamlit as st
import sys
import os
import time
import tempfile
import arxiv
import requests
import re
from collections import Counter

# ================= 1. æ ¸å¿ƒåº“æ£€æŸ¥ =================
try:
    import zhipuai
    import langchain_community
    import fitz  # pymupdf
except ImportError as e:
    st.error(f"ğŸš‘ ç¯å¢ƒç¼ºå¤±åº“ -> {e.name}")
    st.stop()

from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import ZhipuAIEmbeddings
from langchain_community.chat_models import ChatZhipuAI
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ================= 2. å¢å¼ºå‹é¡µé¢é…ç½® =================
st.set_page_config(page_title="AI æ·±åº¦ç ”è¯»åŠ©æ‰‹ (ç§‘ç ”ç‰ˆ)", layout="wide", page_icon="ğŸ“")
st.markdown("""
<style>
    .stButton>button {width: 100%; border-radius: 8px;}
    .abstract-box {
        background-color: #f8f9fa;
        padding: 18px;
        border-radius: 10px;
        border-left: 5px solid #4CAF50;
        font-size: 0.95em;
        line-height: 1.7;
        margin-bottom: 12px;
        color: #2c3e50;
    }
    .cite-badge {
        background-color: #ff4b4b;
        color: white;
        padding: 3px 12px;
        border-radius: 15px;
        font-size: 0.85em;
        font-weight: bold;
    }
    .topic-tag {
        display: inline-block;
        background-color: #e3f2fd;
        color: #1976d2;
        padding: 4px 10px;
        border-radius: 4px;
        margin: 4px;
        font-size: 0.85em;
        border: 1px solid #bbdefb;
    }
</style>
""", unsafe_allow_html=True)
st.title("ğŸ“– AI æ·±åº¦ç ”è¯»åŠ©æ‰‹")

# ================= 3. å…¨å±€çŠ¶æ€åˆå§‹åŒ– =================
# ç¡®ä¿æ‰€æœ‰å˜é‡éƒ½å­˜åœ¨ï¼Œé˜²æ­¢åˆ‡æ¢ Tab æ—¶æŠ¥é”™
state_keys = {
    "chat_history": [],
    "db": None,
    "loaded_files": [],
    "all_chunks": [],
    "suggested_query": "",
    "search_results": [],
    "selected_scope": "ğŸŒ å¯¹æ¯”æ‰€æœ‰è®ºæ–‡"
}
for key, default in state_keys.items():
    if key not in st.session_state:
        st.session_state[key] = default

# ================= 4. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° =================

def fetch_citations(arxiv_id):
    """æ¥å…¥ Semantic Scholar æ•°æ®æµ"""
    try:
        clean_id = arxiv_id.split('/')[-1].split('v')[0]
        # å¢åŠ å»¶è¿Ÿé˜²æ­¢è¢«å°ï¼Œå¢åŠ  influentialCitationCount (é«˜å½±å“åŠ›å¼•ç”¨)
        api_url = f"https://api.semanticscholar.org/graph/v1/paper/ArXiv:{clean_id}?fields=citationCount"
        response = requests.get(api_url, timeout=4)
        if response.status_code == 200:
            return response.json().get('citationCount', 0)
    except:
        pass
    return 0

def extract_top_topics(results):
    """æ¨¡æ‹Ÿè°·æ­Œæœç´¢çš„å…³é”®è¯çƒ­åº¦æå–"""
    all_text = ""
    for item in results:
        res = item['obj']
        all_text += f" {res.title} {res.summary}"
    
    # æ¸…æ´—æ–‡æœ¬ï¼šåªä¿ç•™é•¿äº 5 çš„å•è¯
    words = re.findall(r'\b\w{5,}\b', all_text.lower())
    stop_words = {'learning', 'robotics', 'education', 'research', 'paper', 'approach', 'system', 'based', 'using', 'results', 'study', 'provide', 'performance'}
    meaningful_words = [w for w in words if w not in stop_words]
    return Counter(meaningful_words).most_common(10)

def fix_latex_errors(text):
    """ä¿ç•™å®Œæ•´çš„ LaTeX ä¿®å¤é€»è¾‘"""
    if not text: return text
    text = text.replace(r"\(", "$").replace(r"\)", "$")
    text = text.replace(r"\[", "$$").replace(r"\]", "$$")
    return text

def generate_html_report(chat_history):
    """æ¢å¤å®Œæ•´çš„ HTML å¯¼å‡ºé€»è¾‘ï¼Œå¸¦ MathJax æ”¯æŒ"""
    html = """<!DOCTYPE html><html><head><meta charset="UTF-8"><title>AI ç ”ç©¶ç¬”è®°</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body { font-family: 'Segoe UI', sans-serif; max-width: 850px; margin: 0 auto; padding: 30px; line-height: 1.6; background-color: #fcfcfc; }
        h1 { color: #2e7d32; border-bottom: 2px solid #2e7d32; padding-bottom: 10px; }
        .message { margin-bottom: 25px; padding: 20px; border-radius: 12px; box-shadow: 0 2px 5px rgba(0,0,0,0.05); }
        .user { background-color: #e3f2fd; border-left: 6px solid #2196f3; }
        .assistant { background-color: #f1f8e9; border-left: 6px solid #4caf50; }
        .system { background-color: #fff3e0; border-left: 6px solid #ff9800; font-style: italic; }
        .role-label { font-weight: bold; margin-bottom: 8px; display: block; text-transform: uppercase; font-size: 0.8em; }
    </style></head><body><h1>ğŸ“ AI æ·±åº¦ç ”è¯»ç¬”è®°</h1>"""
    for msg in chat_history:
        role = msg['role']
        label = "ğŸ§‘â€ğŸ’» æˆ‘" if role == 'user' else "ğŸ¤– AI ç ”ç©¶å‘˜" if role == 'assistant' else "ğŸ”” ç³»ç»Ÿé€šçŸ¥"
        content = msg['content'].replace('\n', '<br>')
        html += f'<div class="message {role}"><span class="role-label">{label}</span>{content}</div>'
    html += "</body></html>"
    return html

def rebuild_index_from_chunks(api_key):
    """åˆ é™¤æ–‡ä»¶åé‡æ„æ•°æ®åº“"""
    if not st.session_state.all_chunks:
        st.session_state.db = None
        return
    embeddings = ZhipuAIEmbeddings(model="embedding-2", api_key=api_key)
    st.session_state.db = FAISS.from_documents(st.session_state.all_chunks, embeddings)

def process_and_add_to_db(file_path, file_name, api_key):
    """ä¿ç•™å®Œæ•´çš„ PDF è§£æé€»è¾‘"""
    try:
        loader = PyPDFLoader(file_path)
        docs = loader.load()
        for doc in docs:
            doc.metadata['source_paper'] = file_name
        
        splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=200, separators=["\n\n", "\n", "ã€‚", ".", " ", ""])
        chunks = splitter.split_documents(docs)
        valid_chunks = [c for c in chunks if len(c.page_content.strip()) > 20]
        
        st.session_state.all_chunks.extend(valid_chunks)
        embeddings = ZhipuAIEmbeddings(model="embedding-2", api_key=api_key)
        
        if st.session_state.db is None:
            st.session_state.db = FAISS.from_documents(valid_chunks, embeddings)
        else:
            st.session_state.db.add_documents(valid_chunks)
        
        if file_name not in st.session_state.loaded_files:
            st.session_state.loaded_files.append(file_name)
        
        st.session_state.chat_history.append({"role": "system_notice", "content": f"ğŸ“š å·²æˆåŠŸåŠ è½½ã€Š{file_name}ã€‹"})
    except Exception as e:
        st.error(f"å¤„ç†å¤±è´¥: {e}")

# ================= 5. ä¾§è¾¹æ  =================
with st.sidebar:
    st.header("ğŸ›ï¸ æ§åˆ¶å°")
    user_api_key = st.text_input("æ™ºè°± API Key", type="password")
    st.markdown("---")
    
    if st.session_state.loaded_files:
        st.subheader("ğŸ—‚ï¸ æ–‡çŒ®åº“ç®¡ç†")
        for file in list(st.session_state.loaded_files):
            col_f1, col_f2 = st.columns([4, 1])
            with col_f1: st.caption(f"ğŸ“„ {file[:20]}...")
            with col_f2:
                if st.button("ğŸ—‘ï¸", key=f"del_{file}"):
                    st.session_state.loaded_files.remove(file)
                    st.session_state.all_chunks = [c for c in st.session_state.all_chunks if c.metadata.get('source_paper') != file]
                    if user_api_key: rebuild_index_from_chunks(user_api_key)
                    st.rerun()
        
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºå…¨éƒ¨", type="primary"):
            st.session_state.db, st.session_state.loaded_files, st.session_state.all_chunks, st.session_state.chat_history = None, [], [], []
            st.rerun()

    st.subheader("âš™ï¸ æ¨¡å¼è®¾ç½®")
    reading_mode = st.radio("é˜…è¯»æ¨¡å¼:", ["ğŸŸ¢ å¿«é€Ÿå›ç­”", "ğŸ“– é€æ®µç²¾è¯» (å…¬å¼å¢å¼º)"], index=1)

    if st.session_state.loaded_files:
        st.markdown("---")
        st.session_state.selected_scope = st.selectbox("ğŸ‘ï¸ ç ”è¯»èŒƒå›´", ["ğŸŒ å¯¹æ¯”æ‰€æœ‰è®ºæ–‡"] + st.session_state.loaded_files)
        
        if st.button("ğŸ“„ å¯¼å‡ºç ”è¯»ç¬”è®°"):
            html_content = generate_html_report(st.session_state.chat_history)
            st.download_button("ä¸‹è½½ HTML ç¬”è®°", html_content, "research_notes.html", "text/html")

    st.markdown("---")
    uploaded_file = st.file_uploader("æœ¬åœ° PDF å¯¼å…¥", type="pdf")
    if uploaded_file and user_api_key and st.button("æ‰§è¡ŒåŠ è½½"):
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(uploaded_file.getvalue())
            process_and_add_to_db(tmp.name, uploaded_file.name, user_api_key)
            os.remove(tmp.name)
            st.rerun()

# ================= 6. ä¸»ç•Œé¢ =================
tab_search, tab_chat = st.tabs(["ğŸ” æ–‡çŒ®è°ƒç ” (å¼•ç”¨ä¸çƒ­ç‚¹)", "ğŸ’¬ æ™ºèƒ½ç ”è¯»ç©ºé—´"])

with tab_search:
    st.subheader("ğŸŒ è·¨åº“å­¦æœ¯è°ƒç ”å¼•æ“")
    col_q, col_sort, col_n = st.columns([3, 1.2, 0.8])
    with col_q:
        q = st.text_input("å…³é”®è¯ (æ”¯æŒè‹±æ–‡)", value=st.session_state.suggested_query, placeholder="ä¾‹å¦‚: robotics education K-12")
    with col_sort:
        sort_rule = st.selectbox("æ’åºè§„åˆ™", ["ğŸ”¥ ç›¸å…³æ€§", "ğŸ“… æ—¶é—´æœ€æ–°", "ğŸ“ˆ å¼•ç”¨é‡ä¼˜å…ˆ"])
    with col_n:
        n = st.number_input("è·å–ç¯‡æ•°", 5, 50, 15)

    if st.button("ğŸš€ å¯åŠ¨æ·±åº¦æ£€ç´¢") and q:
        with st.spinner("æ­£åœ¨æ£€ç´¢å¹¶åˆ†æå­¦æœ¯å…ƒæ•°æ®..."):
            try:
                arxiv_sort = arxiv.SortCriterion.Relevance
                if "æ—¶é—´" in sort_rule: arxiv_sort = arxiv.SortCriterion.SubmittedDate
                
                # è‡ªåŠ¨å¸ƒå°”ä¼˜åŒ–
                refined_q = q if ("AND" in q or '"' in q) else " AND ".join([f"(ti:{w} OR abs:{w})" for w in q.split()])
                
                search = arxiv.Search(query=refined_q, max_results=n, sort_by=arxiv_sort)
                raw_results = list(search.results())
                
                results_with_meta = []
                progress = st.progress(0)
                for idx, res in enumerate(raw_results):
                    cites = fetch_citations(res.entry_id)
                    results_with_meta.append({'obj': res, 'citations': cites})
                    progress.progress((idx + 1) / len(raw_results))
                    time.sleep(0.1) # å®‰å…¨å»¶è¿Ÿ
                
                if "å¼•ç”¨é‡" in sort_rule:
                    results_with_meta.sort(key=lambda x: x['citations'], reverse=True)
                
                st.session_state.search_results = results_with_meta
            except Exception as e:
                st.error(f"æ£€ç´¢ä¸­æ–­: {e}")

    if st.session_state.search_results:
        # è°·æ­Œå¼çƒ­ç‚¹è¯æå–
        topics = extract_top_topics(st.session_state.search_results)
        st.write("ğŸ“Š **é¢†åŸŸçƒ­ç‚¹å›¾è°±** (è¾…åŠ©è¯†åˆ«ç ”ç©¶æ–¹å‘):")
        topic_cols = st.columns(len(topics))
        for i, (word, count) in enumerate(topics):
            topic_cols[i].markdown(f"<div class='topic-tag'>{word} ({count})</div>", unsafe_allow_html=True)
        
        st.markdown("---")
        for i, item in enumerate(st.session_state.search_results):
            res, cites = item['obj'], item['citations']
            # å¼ºåŒ–ç²¾å‡†åŒ¹é…è§†è§‰
            is_high = all(w.lower() in res.title.lower() for w in q.split()[:2])
            
            with st.expander(f"{'ğŸ¯' if is_high else 'ğŸ“„'} #{i+1} {res.title} ({res.published.year})"):
                st.markdown(f"**ğŸ”¥ å¼•ç”¨æ•°:** <span class='cite-badge'>{cites}</span> | **ä¸»ä½œè€…:** {res.authors[0].name}", unsafe_allow_html=True)
                # æ¸…æ´—æ‘˜è¦æ¢è¡Œç¬¦
                clean_abs = res.summary.replace('\n', ' ')
                st.markdown(f"<div class='abstract-box'><b>æ‘˜è¦é¢„è§ˆ:</b><br>{clean_abs}</div>", unsafe_allow_html=True)
                
                c1, c2 = st.columns([1, 1])
                with c1: st.markdown(f"[ğŸ”— ArXiv åŸæ–‡]({res.entry_id})")
                with c2:
                    if st.button(f"â¬‡ï¸ åŠ å…¥ç ”è¯»åº“", key=f"dl_main_{i}"):
                        if user_api_key:
                            with st.spinner("åŒæ­¥è‡³å‘é‡åº“..."):
                                pdf_path = res.download_pdf(dirpath=tempfile.gettempdir())
                                process_and_add_to_db(pdf_path, res.title, user_api_key)
                                st.success("å·²å°±ç»ªï¼Œè½¬åˆ°å¯¹è¯ Tab å³å¯æé—®")
                        else: st.error("è¯·åœ¨ä¾§è¾¹æ å¡«å†™ API Key")

with tab_chat:
    if st.session_state.loaded_files:
        st.caption(f"ğŸ“š ç ”è¯»æ¨¡å¼: {reading_mode} | å½“å‰è®ºæ–‡: {st.session_state.selected_scope}")

    for msg in st.session_state.chat_history:
        if msg["role"] == "system_notice": st.info(msg["content"])
        else:
            with st.chat_message(msg["role"]): st.markdown(msg["content"])

    if prompt := st.chat_input("åŸºäºå·²é€‰æ–‡çŒ®æé—®..."):
        if not st.session_state.db: st.warning("è¯·å…ˆåŠ è½½è‡³å°‘ä¸€ç¯‡è®ºæ–‡")
        else:
            st.session_state.chat_history.append({"role": "user", "content": prompt})
            with st.chat_message("user"): st.write(prompt)
            with st.chat_message("assistant"):
                try:
                    scope = st.session_state.selected_scope
                    f_dict = {"source_paper": scope} if scope != "ğŸŒ å¯¹æ¯”æ‰€æœ‰è®ºæ–‡" else None
                    # MMR æœç´¢ä¿è¯æ£€ç´¢å†…å®¹çš„å¤šæ ·æ€§
                    docs = st.session_state.db.max_marginal_relevance_search(prompt, k=8, fetch_k=20, lambda_mult=0.7, filter=f_dict)
                    
                    context = "\n\n".join([f"ğŸ“„ã€{d.metadata.get('source_paper','?')} P{d.metadata.get('page',0)+1}ã€‘:\n{d.page_content}" for d in docs])
                    
                    llm = ChatZhipuAI(model="glm-4", api_key=user_api_key, temperature=0.1)
                    res = llm.invoke(f"ä½ æ˜¯ä¸€ä½èµ„æ·±ç§‘ç ”ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹ç‰‡æ®µå›ç­”ï¼š\n\n{context}\n\né—®é¢˜ï¼š{prompt}\nè¦æ±‚ï¼šä¸¥è°¨å‡†ç¡®ï¼Œå…¬å¼åŠ¡å¿…ä½¿ç”¨ $ åŒ…è£¹ã€‚")
                    final_ans = fix_latex_errors(res.content)
                    st.write(final_ans)
                    st.session_state.chat_history.append({"role": "assistant", "content": final_ans})
                except Exception as e: st.error(f"ç”Ÿæˆå¤±è´¥: {e}")
