import streamlit as st
import sys
import os
import time
import tempfile
import arxiv
import requests  # æ–°å¢ï¼šç”¨äºè°ƒç”¨ Semantic Scholar API
from streamlit_agraph import agraph, Node, Edge, Config # æ–°å¢ï¼šå›¾è°±åº“

# ================= 1. ç¯å¢ƒå¬è¯Šå™¨ =================
try:
    import zhipuai
    import langchain_community
    import fitz  # pymupdf
except ImportError as e:
    st.error(f"ğŸš‘ ç¯å¢ƒç¼ºå¤±åº“ -> {e.name}")
    st.stop()
# ===============================================

from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import ZhipuAIEmbeddings
from langchain_community.chat_models import ChatZhipuAI
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ================= 2. é¡µé¢é…ç½® =================
st.set_page_config(page_title="AI æ·±åº¦ç ”è¯»åŠ©æ‰‹ (ä¸“ä¸šè°ƒç ”ç‰ˆ)", layout="wide", page_icon="ğŸ“")
st.markdown("""
<style>
    .stButton>button {width: 100%; border-radius: 8px;}
    .reportview-container { margin-top: -2em; }
    .abstract-box {
        background-color: #f0f2f6;
        padding: 15px;
        border-radius: 8px;
        border-left: 5px solid #4CAF50;
        font-size: 0.95em;
        line-height: 1.6;
        margin-bottom: 10px;
    }
    .cite-badge {
        background-color: #ff4b4b;
        color: white;
        padding: 2px 8px;
        border-radius: 12px;
        font-size: 0.8em;
        font-weight: bold;
    }
</style>
""", unsafe_allow_html=True)
st.title("ğŸ“– AI æ·±åº¦ç ”è¯»åŠ©æ‰‹ (ä¸“ä¸šè°ƒç ”ç‰ˆ)")

# ================= 3. çŠ¶æ€åˆå§‹åŒ– =================
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "db" not in st.session_state:
    st.session_state.db = None
if "loaded_files" not in st.session_state:
    st.session_state.loaded_files = []
if "all_chunks" not in st.session_state:
    st.session_state.all_chunks = []
if "suggested_query" not in st.session_state:
    st.session_state.suggested_query = ""
if "search_results" not in st.session_state:
    st.session_state.search_results = []
if "selected_scope" not in st.session_state:
    st.session_state.selected_scope = "ğŸŒ å¯¹æ¯”æ‰€æœ‰è®ºæ–‡"
if "focus_paper_id" not in st.session_state: # æ–°å¢ï¼šç”¨äºè·Ÿè¸ªå›¾è°±å±•ç¤º
    st.session_state.focus_paper_id = None

# ================= 4. æ ¸å¿ƒé€»è¾‘å‡½æ•° =================

def fetch_citations(arxiv_id):
    """ä» Semantic Scholar API è·å–å¼•ç”¨æ•°"""
    try:
        clean_id = arxiv_id.split('/')[-1].split('v')[0]
        api_url = f"https://api.semanticscholar.org/graph/v1/paper/ArXiv:{clean_id}?fields=citationCount,title,year"
        response = requests.get(api_url, timeout=5)
        if response.status_code == 200:
            return response.json().get('citationCount', 0)
    except:
        pass
    return 0

# --- æ–°å¢å›¾è°±æ•°æ®è·å–å‡½æ•° ---
def fetch_graph_data(arxiv_id):
    try:
        clean_id = arxiv_id.split('/')[-1].split('v')[0]
        fields = "title,year,references,citations"
        api_url = f"https://api.semanticscholar.org/graph/v1/paper/ArXiv:{clean_id}?fields={fields}"
        response = requests.get(api_url, timeout=8)
        if response.status_code == 200: return response.json()
    except: pass
    return None

# --- æ–°å¢å›¾è°±æ¸²æŸ“å‡½æ•° ---
def render_connected_graph(data):
    if not data: return st.warning("æ— æ³•è·å–å…³è”æ•°æ®")
    nodes, edges = [], []
    # ä¸­å¿ƒèŠ‚ç‚¹
    nodes.append(Node(id="root", label="Seed Paper", size=25, color="#FF4B4B"))
    # è¢«å¼• (Citations)
    for i, item in enumerate(data.get('citations', [])[:10]):
        nid = f"c_{i}"
        nodes.append(Node(id=nid, label=item.get('title','')[:20], size=15, color="#2ca02c"))
        edges.append(Edge(source=nid, target="root"))
    # å¼•ç”¨ (References)
    for i, item in enumerate(data.get('references', [])[:10]):
        nid = f"r_{i}"
        nodes.append(Node(id=nid, label=item.get('title','')[:20], size=15, color="#1f77b4"))
        edges.append(Edge(source="root", target=nid))
    
    config = Config(width=1000, height=450, directed=True, physics=True)
    return agraph(nodes=nodes, edges=edges, config=config)

def fix_latex_errors(text):
    if not text: return text
    text = text.replace(r"\(", "$").replace(r"\)", "$")
    text = text.replace(r"\[", "$$").replace(r"\]", "$$")
    return text

def rebuild_index_from_chunks(api_key):
    if not st.session_state.all_chunks:
        st.session_state.db = None
        return
    embeddings = ZhipuAIEmbeddings(model="embedding-2", api_key=api_key)
    st.session_state.db = FAISS.from_documents(st.session_state.all_chunks, embeddings)

def process_and_add_to_db(file_path, file_name, api_key):
    try:
        loader = PyPDFLoader(file_path)
        docs = loader.load()
        for doc in docs:
            doc.metadata['source_paper'] = file_name
        
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=600,       
            chunk_overlap=200,    
            separators=["\n\n", "\n", "ã€‚", ".", " ", ""]
        )
        chunks = splitter.split_documents(docs)
        valid_chunks = [c for c in chunks if len(c.page_content.strip()) > 20]
        
        st.session_state.all_chunks.extend(valid_chunks)
        
        embeddings = ZhipuAIEmbeddings(model="embedding-2", api_key=api_key)
        
        batch_size = 10
        total = len(valid_chunks)
        if st.session_state.db is None:
            st.session_state.db = FAISS.from_documents(valid_chunks[:batch_size], embeddings)
            if total > batch_size:
                for i in range(batch_size, total, batch_size):
                    st.session_state.db.add_documents(valid_chunks[i: i + batch_size])
                    time.sleep(0.1)
        else:
            for i in range(0, total, batch_size):
                st.session_state.db.add_documents(valid_chunks[i: i + batch_size])
                time.sleep(0.1)
        
        if file_name not in st.session_state.loaded_files:
            st.session_state.loaded_files.append(file_name)
        
        st.session_state.chat_history.append({
            "role": "system_notice",
            "content": f"ğŸ“š **ç³»ç»Ÿé€šçŸ¥**ï¼šå·²åŠ è½½ã€Š{file_name}ã€‹ã€‚"
        })
    except Exception as e:
        st.error(f"å¤„ç†å¤±è´¥: {e}")

def generate_html_report(chat_history):
    html = """<!DOCTYPE html><html><head><meta charset="UTF-8"><title>AI ç ”ç©¶ç¬”è®°</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>body { font-family: sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; line-height: 1.6; }
    .message { margin-bottom: 20px; padding: 15px; border-radius: 8px; }
    .user { background-color: #e3f2fd; border-left: 5px solid #2196F3; }
    .assistant { background-color: #f1f8e9; border-left: 5px solid #4CAF50; }</style></head>
    <body><h1>ğŸ“ AI æ·±åº¦ç ”è¯»ç¬”è®°</h1><p>å¯¼å‡ºæ—¶é—´ï¼š""" + time.strftime('%Y-%m-%d %H:%M') + """</p>"""
    for msg in chat_history:
        role_class = msg['role'] if msg['role'] in ['user', 'assistant'] else 'system'
        content_html = msg['content'].replace('\n', '<br>')
        html += f'<div class="message {role_class}"><b>{msg["role"]}</b><br>{content_html}</div>'
    html += "</body></html>"
    return html

# ================= 5. ä¾§è¾¹æ  =================
with st.sidebar:
    st.header("ğŸ›ï¸ æ§åˆ¶å°")
    user_api_key = st.text_input("æ™ºè°± API Key", type="password")
    st.markdown("---")
    
    if st.session_state.loaded_files:
        st.subheader("ğŸ—‚ï¸ æ–‡ä»¶ç®¡ç†")
        for file in list(st.session_state.loaded_files):
            col_f1, col_f2 = st.columns([4, 1])
            with col_f1: st.text(f"ğŸ“„ {file[:18]}..." if len(file)>20 else f"ğŸ“„ {file}")
            with col_f2:
                if st.button("ğŸ—‘ï¸", key=f"del_{file}"):
                    st.session_state.loaded_files.remove(file)
                    st.session_state.all_chunks = [c for c in st.session_state.all_chunks if c.metadata.get('source_paper') != file]
                    if user_api_key: rebuild_index_from_chunks(user_api_key)
                    st.rerun()
        
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºå…¨éƒ¨", type="primary"):
            st.session_state.db = None
            st.session_state.loaded_files = []
            st.session_state.all_chunks = []
            st.session_state.chat_history = []
            st.rerun()

    st.subheader("âš™ï¸ ç ”è¯»æ¨¡å¼")
    reading_mode = st.radio("é€‰æ‹©æ¨¡å¼:", ["ğŸŸ¢ å¿«é€Ÿé—®ç­”", "ğŸ“– é€æ®µç²¾è¯» (å…¬å¼ä¿®å¤ç‰ˆ)"], index=1)

    if st.session_state.loaded_files:
        st.markdown("---")
        if st.button("ğŸª„ ä¸€é”®ç”Ÿæˆç»¼è¿°å¯¹æ¯”è¡¨"):
            if user_api_key and st.session_state.db:
                with st.spinner("åˆ†æä¸­..."):
                    llm = ChatZhipuAI(model="glm-4", api_key=user_api_key, temperature=0.1)
                    aggregated_context = ""
                    for filename in st.session_state.loaded_files:
                        sub_docs = st.session_state.db.similarity_search("Abstract conclusion main contribution", k=3, filter={"source_paper": filename})
                        if sub_docs: aggregated_context += f"\n=== {filename} ===\n" + "\n".join([d.page_content for d in sub_docs]) + "\n"
                    res = llm.invoke(f"é˜…è¯»ä»¥ä¸‹æ‘˜è¦ï¼Œç”Ÿæˆ Markdown è¡¨æ ¼(åˆ—ï¼šè®ºæ–‡å|åˆ›æ–°ç‚¹|æ–¹æ³•|ç»“è®º)ï¼š\n{aggregated_context}")
                    st.session_state.chat_history.append({"role": "assistant", "content": res.content})
                    st.rerun()

        scope_options = ["ğŸŒ å¯¹æ¯”æ‰€æœ‰è®ºæ–‡"] + st.session_state.loaded_files
        st.session_state.selected_scope = st.selectbox("ğŸ‘ï¸ ä¸“æ³¨èŒƒå›´", scope_options)

    st.markdown("---")
    st.subheader("ğŸ“¥ ä¸Šä¼ è®ºæ–‡")
    uploaded_file = st.file_uploader("æ‹–å…¥ PDF", type="pdf")
    if uploaded_file and user_api_key and st.button("ç¡®è®¤åŠ è½½"):
        with st.spinner("è§£æä¸­..."):
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
                tmp.write(uploaded_file.getvalue())
                path = tmp.name
            process_and_add_to_db(path, uploaded_file.name, user_api_key)
            os.remove(path)
            st.rerun()

# ================= 6. ä¸»ç•Œé¢ =================
tab_search, tab_chat = st.tabs(["ğŸ” æ–‡çŒ®è°ƒç ” (å¼•ç”¨å¢å¼º)", "ğŸ’¬ ç ”è¯»ç©ºé—´"])

with tab_search:
    st.subheader("ğŸŒ å­¦æœ¯å¤§æ•°æ®æ£€ç´¢")
    col_q, col_sort, col_n = st.columns([3, 1.5, 1])
    with col_q:
        search_query = st.text_input("å…³é”®è¯", value=st.session_state.suggested_query, placeholder="ä¾‹å¦‚: education robot")
    with col_sort:
        sort_mode = st.selectbox("æ’åºè§„åˆ™", ["ğŸ”¥ ç›¸å…³æ€§ä¼˜å…ˆ", "ğŸ“… æ—¶é—´ç”±æ–°åˆ°æ—§", "ğŸ“ˆ å¼•ç”¨é‡ç”±é«˜åˆ°ä½"])
    with col_n:
        max_results = st.number_input("è·å–æ•°é‡", min_value=5, max_value=50, value=15)
        
    if st.button("ğŸš€ å¼€å§‹æ£€ç´¢") and search_query:
        with st.spinner("æ­£åœ¨æ£€ç´¢å¹¶åŒæ­¥ Semantic Scholar å¼•ç”¨æ•°æ®..."):
            try:
                # ArXiv æ’åºå‚æ•°æ˜ å°„
                arxiv_sort = arxiv.SortCriterion.Relevance
                if "æ—¶é—´" in sort_mode: arxiv_sort = arxiv.SortCriterion.SubmittedDate
                
                # è‡ªåŠ¨ä¼˜åŒ–å¸ƒå°”æŸ¥è¯¢
                refined_query = search_query
                if " " in search_query and "AND" not in search_query and '"' not in search_query:
                    refined_query = " AND ".join([f'(ti:{w} OR abs:{w})' for w in search_query.split()])

                search = arxiv.Search(query=refined_query, max_results=max_results, sort_by=arxiv_sort)
                raw_results = list(search.results())
                
                # å¼•ç”¨æ•°è¡¥å…¨
                results_with_cite = []
                progress_bar = st.progress(0)
                for idx, res in enumerate(raw_results):
                    cites = fetch_citations(res.entry_id)
                    results_with_cite.append({'obj': res, 'citations': cites})
                    progress_bar.progress((idx + 1) / len(raw_results))
                
                # å¼•ç”¨æ’åºå¤„ç†
                if "å¼•ç”¨é‡" in sort_mode:
                    results_with_cite.sort(key=lambda x: x['citations'], reverse=True)
                
                st.session_state.search_results = results_with_cite
                st.success(f"âœ… å®Œæˆï¼å·²è·å– {len(results_with_cite)} ç¯‡è®ºæ–‡ã€‚")
            except Exception as e:
                st.error(f"æ£€ç´¢å¤±è´¥: {e}")
                
    if st.session_state.search_results:
        # æ–°å¢å›¾è°±æ˜¾ç¤ºåŒºåŸŸ
        if st.session_state.focus_paper_id:
            st.markdown("---")
            st.subheader("ğŸ“Š æ–‡çŒ®å…³è”å›¾è°± (Connected Graph)")
            col_graph, col_info = st.columns([3, 1])
            with col_graph:
                g_data = fetch_graph_data(st.session_state.focus_paper_id)
                render_connected_graph(g_data)
            with col_info:
                st.caption("ğŸŸ¢ ç»¿è‰²: Citations (å¼•ç”¨æœ¬æ–‡)")
                st.caption("ğŸ”µ è“è‰²: References (å‚è€ƒæ–‡çŒ®)")
                if st.button("âŒ å…³é—­å›¾è°±"):
                    st.session_state.focus_paper_id = None
                    st.rerun()
            st.markdown("---")

        for i, item in enumerate(st.session_state.search_results):
            res = item['obj']
            cites = item['citations']
            with st.expander(f"#{i+1} ğŸ“„ {res.title} ({res.published.year})"):
                st.markdown(f"**ğŸ‘¨â€ğŸ« ä½œè€…**: {', '.join([a.name for a in res.authors])} | **ğŸ“… å‘è¡¨**: {res.published.strftime('%Y-%m-%d')}")
                st.markdown(f"**ğŸ”¥ å¼•ç”¨æ•° (Semantic Scholar)**: <span class='cite-badge'>{cites}</span>", unsafe_allow_html=True)
                
                st.markdown(f'<div class="abstract-box"><b>ğŸ“ æ‘˜è¦ï¼š</b><br>{res.summary.replace("\n", " ")}</div>', unsafe_allow_html=True)
                
                col1, col2, col3 = st.columns([1, 1, 1])
                with col1: st.markdown(f"[ğŸ”— ArXiv åŸæ–‡]({res.entry_id})")
                with col2:
                    if st.button(f"â¬‡ï¸ ä¸‹è½½åˆ†æ", key=f"dl_search_{i}"):
                        if user_api_key:
                            with st.spinner("ä¸‹è½½è§£æä¸­..."):
                                try:
                                    pdf_path = res.download_pdf(dirpath=tempfile.gettempdir())
                                    process_and_add_to_db(pdf_path, res.title, user_api_key)
                                    st.success("å…¥åº“æˆåŠŸï¼")
                                except Exception as e: st.error(f"å¤±è´¥: {e}")
                        else: st.error("è¯·å¡«å…¥ API Key")
                with col3:
                    if st.button(f"ğŸ•¸ï¸ å…³è”å›¾è°±", key=f"btn_graph_{i}"):
                        st.session_state.focus_paper_id = res.entry_id
                        st.rerun()

with tab_chat:
    if st.session_state.loaded_files:
        st.caption(f"ğŸ“š æ¨¡å¼ï¼š{reading_mode} | èŒƒå›´ï¼š{st.session_state.selected_scope}")

    for msg in st.session_state.chat_history:
        if msg["role"] == "system_notice": st.info(msg["content"])
        else:
            with st.chat_message(msg["role"]): st.markdown(msg["content"])

    if prompt := st.chat_input("è¾“å…¥é—®é¢˜..."):
        if not st.session_state.db: st.warning("ğŸ§  è¯·å…ˆæ·»åŠ è®ºæ–‡")
        else:
            st.session_state.chat_history.append({"role": "user", "content": prompt})
            with st.chat_message("user"): st.write(prompt)
            with st.chat_message("assistant"):
                try:
                    search_k = 15 if "ç²¾è¯»" in reading_mode else 8
                    current_scope = st.session_state.get("selected_scope", "ğŸŒ å¯¹æ¯”æ‰€æœ‰è®ºæ–‡")
                    filter_dict = {"source_paper": current_scope} if current_scope != "ğŸŒ å¯¹æ¯”æ‰€æœ‰è®ºæ–‡" else None

                    docs = st.session_state.db.max_marginal_relevance_search(prompt, k=search_k, fetch_k=20, lambda_mult=0.6, filter=filter_dict)
                    if not docs: st.warning("æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚")
                    else:
                        context = "\n\n".join([f"ğŸ“„ã€{d.metadata.get('source_paper','?')} P{d.metadata.get('page',0)+1}ã€‘:\n{d.page_content}" for d in docs])
                        sys_prompt = f"ä½ æ˜¯ä¸€ä½ç§‘ç ”åŠ©æ‰‹ã€‚åŸºäºèµ„æ–™å›ç­”é—®é¢˜ï¼š\nèµ„æ–™ï¼š{context}\né—®é¢˜ï¼š{prompt}\nè¦æ±‚ï¼šå…¬å¼ç”¨ $ åŒ…è£¹ã€‚"
                        llm = ChatZhipuAI(model="glm-4", api_key=user_api_key, temperature=0.1)
                        response = llm.invoke(sys_prompt)
                        final_content = fix_latex_errors(response.content)
                        st.write(final_content)
                        st.session_state.chat_history.append({"role": "assistant", "content": final_content})
                except Exception as e: st.error(f"ç”Ÿæˆå‡ºé”™: {e}")
